# Import necessary libraries
import torch
from torch.utils.data import DataLoader
from torchvision.transforms import ToTensor, Normalize, Compose
from flwr_datasets import FederatedDataset
from flwr_datasets.partitioner import IidPartitioner, DirichletPartitioner
from flwr.client import NumPyClient
from flwr.common import Context, NDArrays, Scalar, ndarrays_to_parameters
from flwr.server import ServerApp, ServerConfig, ServerAppComponents
from flwr.server.strategy import FedAvg
from flwr.simulation import run_simulation
from collections import OrderedDict
from typing import Dict, Tuple, List
import torch.nn as nn
import torch.nn.functional as F
from sklearn.metrics import cohen_kappa_score, f1_score, roc_auc_score
from sklearn.preprocessing import label_binarize
from flwr.server.client_proxy import ClientProxy
import json
import os
import random
import numpy as np
import matplotlib.pyplot as plt
from logging import INFO
from flwr.common.logger import log

# Set random seeds for reproducibility
def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed()

# Define constants
NUM_CLIENTS = 5
NUM_ROUNDS = 50
BATCH_SIZE = 32

# Define the number of attackers
ATTACKER_IDS_LIST = [
    {"scenario": "Baseline_No_Attacks", "attackers": []},
    {"scenario": "Single_Attacker", "attackers": [0]},
    {"scenario": "Multiple_Attackers", "attackers": [0, 1]},
]

# Define partitioners for IID and non-IID
PARTITIONERS = {
    "IID": IidPartitioner(num_partitions=NUM_CLIENTS),
    "NonIID": DirichletPartitioner(num_partitions=NUM_CLIENTS, alpha=0.1, partition_by="label"),
}

# Define the neural network model suitable for CIFAR-10
class Net(nn.Module):
    """Model (simple CNN adapted for CIFAR-10)"""

    def __init__(self, num_classes: int = 10):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)  # Adjusted for CIFAR-10's 3 channels
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.fc1 = nn.Linear(64 * 8 * 8, 512)  # CIFAR-10 images are 32x32
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, num_classes)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.pool(F.relu(self.conv1(x)))  # [batch, 32, 16, 16]
        x = self.pool(F.relu(self.conv2(x)))  # [batch, 64, 8, 8]
        x = x.view(-1, 64 * 8 * 8)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Functions to set and get model parameters
def set_params(model: nn.Module, parameters: NDArrays):
    """Replace model parameters with those passed as parameters."""
    params_dict = zip(model.state_dict().keys(), parameters)
    state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})
    model.load_state_dict(state_dict, strict=True)

def get_params(model: nn.Module) -> NDArrays:
    """Extract model parameters as a list of NumPy arrays."""
    return [val.cpu().numpy() for _, val in model.state_dict().items()]

# Training function with label flipping for attackers
def train(net: nn.Module, trainloader: DataLoader, optimizer: torch.optim.Optimizer, device: str = "cpu", is_attacker: bool = False):
    """Train the network on the training set."""
    criterion = torch.nn.CrossEntropyLoss()
    net.to(device)
    net.train()
    for batch in trainloader:
        images, labels = batch["img"].to(device), batch["label"].to(device)
        optimizer.zero_grad()
        if is_attacker:
            # Flip labels for attackers (simple strategy: label = (label + 1) % 10)
            labels = (labels + 1) % 10
        outputs = net(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

# Testing function with metric calculations
def test(net: nn.Module, testloader: DataLoader, device: str):
    """Validate the network on the entire test set."""
    criterion = torch.nn.CrossEntropyLoss()
    net.to(device)
    net.eval()
    correct, loss = 0, 0.0
    all_preds = []
    all_labels = []
    all_outputs = []
    with torch.no_grad():
        for batch in testloader:
            images, labels = batch["img"].to(device), batch["label"].to(device)
            outputs = net(images)
            loss += criterion(outputs, labels).item()
            _, predicted = torch.max(outputs.data, 1)
            correct += (predicted == labels).sum().item()
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_outputs.extend(outputs.cpu().numpy())
    accuracy = correct / len(testloader.dataset)

    # Compute metrics
    kappa = cohen_kappa_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='macro')
    # Binarize labels for ROC AUC
    all_labels_bin = label_binarize(all_labels, classes=list(range(10)))
    all_outputs_array = np.array(all_outputs)
    roc_auc = roc_auc_score(all_labels_bin, all_outputs_array, average='macro', multi_class='ovr')

    metrics = {
        "accuracy": accuracy,
        "kappa": kappa,
        "f1_score": f1,
        "roc_auc": roc_auc,
    }
    return loss, accuracy, metrics

# Define the FlowerClient class
class FlowerClient(NumPyClient):
    def __init__(self, trainloader: DataLoader, valloader: DataLoader, is_attacker: bool = False) -> None:
        super().__init__()
        self.trainloader = trainloader
        self.valloader = valloader
        self.model = Net(num_classes=10)
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.is_attacker = is_attacker

    def fit(self, parameters: NDArrays, config: Dict[str, Scalar]) -> Tuple[NDArrays, int, Dict]:
        """Train the model locally."""
        # Set model parameters
        set_params(self.model, parameters)

        # Define the optimizer
        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01, momentum=0.9)

        # Train the model
        train(self.model, self.trainloader, optimizer, self.device, is_attacker=self.is_attacker)

        # Return updated parameters
        return get_params(self.model), len(self.trainloader.dataset), {}

    def evaluate(self, parameters: NDArrays, config: Dict[str, Scalar]) -> Tuple[float, int, Dict]:
        """Evaluate the model locally."""
        set_params(self.model, parameters)
        loss, accuracy, metrics = test(self.model, self.valloader, self.device)
        return float(loss), len(self.valloader.dataset), metrics

# Define the client function
def client_fn(context: Context):
    """Create a Flower client representing a participant in the federated learning."""
    partition_id = int(context.node_config["partition-id"])
    partition = fds.load_partition(partition_id, "train")
    # Partition into train/validation
    partition_train_val = partition.train_test_split(test_size=0.1, seed=42)
    # Get dataloaders
    trainloader = get_mnist_dataloaders(partition_train_val["train"], batch_size=BATCH_SIZE)
    valloader = get_mnist_dataloaders(partition_train_val["test"], batch_size=BATCH_SIZE)
    # Determine if the client is an attacker
    is_attacker = partition_id in ATTACKER_IDS
    return FlowerClient(trainloader=trainloader, valloader=valloader, is_attacker=is_attacker).to_client()

# Function to load data correctly for CIFAR-10
fds = None  # Cache FederatedDataset

def load_data(partition_id: int, num_partitions: int, batch_size: int):
    """Load partition CIFAR10 data."""
    global fds
    if fds is None:
        partitioner = current_partitioner  # Set dynamically based on scenario
        fds = FederatedDataset(
            dataset="uoft-cs/cifar10",
            partitioners={"train": partitioner},
        )
    partition = fds.load_partition(partition_id, "train")
    # Divide data on each node: 90% train, 10% test for validation
    partition_train_val = partition.train_test_split(test_size=0.1, seed=42)
    pytorch_transforms = Compose(
        [ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]
    )

    def apply_transforms(batch):
        """Apply transforms to the partition from FederatedDataset."""
        batch["img"] = [pytorch_transforms(img) for img in batch["img"]]
        return batch

    partition_train_val = partition_train_val.with_transform(apply_transforms)
    trainloader = DataLoader(
        partition_train_val["train"], batch_size=batch_size, shuffle=True
    )
    testloader = DataLoader(partition_train_val["test"], batch_size=batch_size)
    return trainloader, testloader

def get_mnist_dataloaders(dataset_partition, batch_size: int):
    """Utility function to create dataloaders."""
    return DataLoader(dataset_partition, batch_size=batch_size, shuffle=True)

# Define custom strategy to log metrics
class CustomFedAvg(FedAvg):
    def __init__(self, metrics_file: str, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.metrics_file = metrics_file
        self.metrics = {}

    def aggregate_evaluate(
        self,
        rnd: int,
        results: List[Tuple[ClientProxy, Tuple[float, int, Dict]]],
        failures: List[BaseException],
    ):
        """Aggregate evaluation results using weighted average and log metrics per round."""
        if not results:
            return None, {}
        # Use weighted average to aggregate metrics
        num_examples_total = sum([r[1][1] for r in results])
        accuracy = sum([r[1][1] * r[1][2]["accuracy"] for r in results]) / num_examples_total
        kappa = sum([r[1][1] * r[1][2]["kappa"] for r in results]) / num_examples_total
        f1 = sum([r[1][1] * r[1][2]["f1_score"] for r in results]) / num_examples_total
        roc_auc = sum([r[1][1] * r[1][2]["roc_auc"] for r in results]) / num_examples_total

        # Log metrics
        log(INFO, f"Round {rnd} evaluation metrics:")
        log(INFO, f"Accuracy: {accuracy:.4f}")
        log(INFO, f"Kappa: {kappa:.4f}")
        log(INFO, f"F1 Score: {f1:.4f}")
        log(INFO, f"ROC AUC: {roc_auc:.4f}")

        # Store metrics
        self.metrics[rnd] = {
            "accuracy": accuracy,
            "kappa": kappa,
            "f1_score": f1,
            "roc_auc": roc_auc,
        }

        # Save metrics to a JSON file
        with open(self.metrics_file, "w") as f:
            json.dump(self.metrics, f, indent=4)

        # Optionally, you can also print or plot metrics here

        # Return aggregated loss and metrics
        return super().aggregate_evaluate(rnd, results, failures)

# Define the server function
def server_fn(context: Context, scenario_name: str):
    # Instantiate the model
    model = Net(num_classes=10)
    ndarrays = get_params(model)
    # Convert model parameters to flwr.common.Parameters
    global_model_init = ndarrays_to_parameters(ndarrays)

    # Define the strategy with a unique metrics file
    strategy = CustomFedAvg(
        fraction_fit=1.0,        # All clients participate in training
        fraction_evaluate=1.0,   # All clients participate in evaluation
        initial_parameters=global_model_init,  # Initialized global model
        metrics_file=f"metrics_{scenario_name}.json",  # Unique metrics file per scenario
    )

    # Construct ServerConfig
    config = ServerConfig(num_rounds=NUM_ROUNDS)

    # Wrap everything into a ServerAppComponents object
    return ServerAppComponents(strategy=strategy, config=config)

# Define a custom ServerApp to pass scenario_name
class FlowerServerApp(ServerApp):
    def __init__(self, server_fn, scenario_name, *args, **kwargs):
        self.scenario_name = scenario_name
        super().__init__(server_fn=lambda context: server_fn(context, self.scenario_name), *args, **kwargs)

# Define the simulation runner
def run_federated_simulation(distribution: str, scenario: Dict):
    """Run federated learning simulation for a given data distribution and attack scenario."""
    global current_partitioner, ATTACKER_IDS

    # Set the current partitioner based on distribution
    current_partitioner = PARTITIONERS[distribution]

    # Initialize FederatedDataset with the current partitioner
    global fds
    fds = FederatedDataset(
        dataset="uoft-cs/cifar10",
        partitioners={"train": current_partitioner},
    )

    # Set attacker IDs for this scenario
    ATTACKER_IDS = scenario["attackers"]

    # Define scenario name
    scenario_name = f"{distribution}_{scenario['scenario']}"

    print(f"\nRunning simulation: {scenario_name} with attackers {ATTACKER_IDS}")

    # Instantiate the server app with the scenario name
    server_app = FlowerServerApp(server_fn=server_fn, scenario_name=scenario_name)

    # Run the simulation
    run_simulation(
        server_app=server_app,
        client_fn=client_fn,
        num_supernodes=NUM_CLIENTS,
        backend_name="ray",
        verbose_logging=True,
    )

# Main execution
if __name__ == "__main__":
    # Define simulation scenarios for IID and NonIID distributions
    distributions = ["IID", "NonIID"]

    # Run simulations for each data distribution and attack scenario
    for distribution in distributions:
        print(f"\n=== Starting simulations for {distribution} data distribution ===")
        for scenario in ATTACKER_IDS_LIST:
            run_federated_simulation(distribution, scenario)

    # After simulations, you can load and visualize the metrics
    # Example visualization (optional)
    metrics_types = ["accuracy", "kappa", "f1_score", "roc_auc"]
    for metric in metrics_types:
        plt.figure(figsize=(10, 6))
        for distribution in distributions:
            for scenario in ATTACKER_IDS_LIST:
                scenario_name = f"{distribution}_{scenario['scenario']}"
                metrics_file = f"metrics_{scenario_name}.json"
                if os.path.exists(metrics_file):
                    with open(metrics_file, "r") as f:
                        metrics = json.load(f)
                    rounds = sorted(metrics.keys(), key=lambda x: int(x))
                    values = [metrics[rnd][metric] for rnd in rounds]
                    plt.plot(rounds, values, label=f"{distribution} - {scenario['scenario']}")
        plt.title(f"{metric.capitalize()} over Communication Rounds")
        plt.xlabel("Round")
        plt.ylabel(metric.capitalize())
        plt.legend()
        plt.savefig(f"{metric}_comparison.png")
        plt.close()

    print("\nSimulations completed. Metrics have been saved and plots generated.")
